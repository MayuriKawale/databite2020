{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_SentimentAnalysis_LSTMs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqRJ_QfFlQNP"
      },
      "source": [
        "In this notebook, we are using TensorFlow keras packages to classify the movie reviews from IMDB dataset. \n",
        "TensorFlow provides a preprocessed [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) which consists 50000 movie reviews. The positive and negative reviews are equal in number. This dataset contains preprocessed movie reviews in the form of sequence of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhoSw249nbsQ"
      },
      "source": [
        "# Required import statements\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM, Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import one_hot"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAHv2vJt9Cda"
      },
      "source": [
        "**1. Download the IMDB dataset from keras datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p7hOW7foNcj"
      },
      "source": [
        "# Download the imdb dataset\n",
        "'''\n",
        "load_data will load the preprocessed imdb dataset\n",
        "num_words argument is used to define the top most frequent words in the training data\n",
        "Here num_words = 1000, means top 1000 most frequent words \n",
        "We will split the data into train and test \n",
        "'''\n",
        "top_words = 1000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AAV0I2k9SSk"
      },
      "source": [
        "**2. Review the size of Train and Test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyPVy7qBpeEQ",
        "outputId": "4b424a1e-ed9a-4a64-825c-83772a917b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The size of train data\n",
        "X_train.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3mvAG4apVmq",
        "outputId": "5fb7ac0b-45e5-414c-d19d-481caae2c80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The size of test data\n",
        "y_test.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Uha-bTpofa",
        "outputId": "4e339bd2-79eb-4274-d95d-30903489dbfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# The preprocessed data will have each integer representing a word in dictionary.\n",
        "# The first review will look like this:\n",
        "print(X_train[0])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK6gM11X9ukO"
      },
      "source": [
        "**3. Sequence of integers back to original words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rjD3o2_uthy",
        "outputId": "40df8a03-341f-4481-e6c9-371132f95455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "Mapping the movie review in form of sequence of integers back to original words\n",
        "'''\n",
        "word_integer = imdb.get_word_index()\n",
        "\n",
        "integer_word = {i: word for word, i in word_integer.items()}\n",
        "\n",
        "print([integer_word.get(i, ' ') for i in X_train[0]])\n",
        "\n",
        "'''\n",
        "Printing reviews converted to original words\n",
        "'''\n",
        "def review_to_word(sentence):\n",
        "  return ([integer_word.get(i, ' ') for i in sentence])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'and', 'and', 'their', 'becomes', 'and', 'had', 'and', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'and', 'and', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'and', 'film', 'as', 'you', 'of', 'and', 'and', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'and', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', 'and', 'and', 'with', 'heart', 'had', 'and', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'and', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', 'and', 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'and', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'and', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'and', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'and', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olpWMrFeALAi",
        "outputId": "fb347477-9988-466f-a2fc-a7fa0a9530eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Using the method review to word\n",
        "\n",
        "print(review_to_word(X_train[0]))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'and', 'and', 'their', 'becomes', 'and', 'had', 'and', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'and', 'and', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'and', 'film', 'as', 'you', 'of', 'and', 'and', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'and', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', 'and', 'and', 'with', 'heart', 'had', 'and', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'and', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', 'and', 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'and', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'and', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'and', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'and', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbfugmgyqc0z"
      },
      "source": [
        "'''\n",
        "All the reviews in the movie dataset are of different length and in the form of array of integers\n",
        "Neural network only accepts the input of same length (Use pad_sequences function from keras.preprocessing)\n",
        "The same length integer reviews can be used to create tensor for input to the neural network\n",
        "Using the argument padding = 'post' will add the extra 0 at the end of the review\n",
        "'''\n",
        "\n",
        "max_review_words = 250\n",
        "\n",
        "train_encode = sequence.pad_sequences(X_train, \n",
        "                                      padding='post', \n",
        "                                      maxlen=max_review_words)\n",
        "test_encode = sequence.pad_sequences(X_test, \n",
        "                                     padding='post', \n",
        "                                     maxlen=max_review_words)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IGFqvw4CvTw",
        "outputId": "a65a4be2-b958-4387-c8b5-72e0c20db8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The length of reviews\n",
        "len(train_encode[0]), len(test_encode[1])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LptObekC0q3",
        "outputId": "f48265be-4c1a-44e2-88c9-6fde89d2ea42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# the updated movie review that is padded\n",
        "print(train_encode[0])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1  14  22  16  43 530 973   2   2  65 458   2  66   2   4 173  36 256\n",
            "   5  25 100  43 838 112  50 670   2   9  35 480 284   5 150   4 172 112\n",
            " 167   2 336 385  39   4 172   2   2  17 546  38  13 447   4 192  50  16\n",
            "   6 147   2  19  14  22   4   2   2 469   4  22  71  87  12  16  43 530\n",
            "  38  76  15  13   2   4  22  17 515  17  12  16 626  18   2   5  62 386\n",
            "  12   8 316   8 106   5   4   2   2  16 480  66   2  33   4 130  12  16\n",
            "  38 619   5  25 124  51  36 135  48  25   2  33   6  22  12 215  28  77\n",
            "  52   5  14 407  16  82   2   8   4 107 117   2  15 256   4   2   7   2\n",
            "   5 723  36  71  43 530 476  26 400 317  46   7   4   2   2  13 104  88\n",
            "   4 381  15 297  98  32   2  56  26 141   6 194   2  18   4 226  22  21\n",
            " 134 476  26 480   5 144  30   2  18  51  36  28 224  92  25 104   4 226\n",
            "  65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19\n",
            " 178  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNVENvsZDd3v"
      },
      "source": [
        "**4. Building Model and Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZNcmUnXDHdd",
        "outputId": "1d368f8f-ae34-4151-ec52-c98f57761d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "'''\n",
        "To build a neural network we need decide the number of layers to use in the model\n",
        "Also the number of hidden units for each layer\n",
        "'''\n",
        "\n",
        "# defining parameters\n",
        "vocab_size = 5000\n",
        "Max_Words_Review = 250\n",
        "embedding_vector_length = 32 \n",
        "epochs = 20\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#The first layer is an Embedding layer. \n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length = Max_Words_Review))\n",
        "#Add convolutional layer that has 32 feature maps and \n",
        "# reads embedded word representations 3 vector elements of the word embedding at a time.\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# The last layer is densely connected with a single output node. \n",
        "# Using the sigmoid activation function, representing a probabilty (float value between 0 and 1).\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "#Loss function and optimizer\n",
        "opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(loss='binary_crossentropy', optimizer= opt, metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                               min_delta=0.0001, patience= 10, \n",
        "                                               verbose=0, mode='auto', \n",
        "                                               baseline=None, \n",
        "                                               restore_best_weights=True)\n",
        "\n",
        "x_val = train_encode[:10000]\n",
        "train_set_x = train_encode[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "train_set_y = y_train[10000:]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 250, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 250, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 125, 32)           0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 125, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 150)               109800    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                2416      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 275,337\n",
            "Trainable params: 275,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CViFS4wgI0v"
      },
      "source": [
        "**5. Train the model for sentiment analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMqY4V9qgHLk",
        "outputId": "42b02161-6161-4730-e8dd-80853e4c696e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model.fit(x=np.array(train_set_x), y=np.array(train_set_y),\n",
        "          epochs=epochs, \n",
        "          validation_data=(x_val, y_val),\n",
        "          callbacks=[early_stopping], batch_size=64)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "235/235 [==============================] - 83s 351ms/step - loss: 0.6931 - accuracy: 0.5051 - val_loss: 0.6931 - val_accuracy: 0.5135\n",
            "Epoch 2/20\n",
            "235/235 [==============================] - 82s 349ms/step - loss: 0.6926 - accuracy: 0.5167 - val_loss: 0.6914 - val_accuracy: 0.5319\n",
            "Epoch 3/20\n",
            "235/235 [==============================] - 82s 349ms/step - loss: 0.6891 - accuracy: 0.5354 - val_loss: 0.6807 - val_accuracy: 0.5516\n",
            "Epoch 4/20\n",
            "235/235 [==============================] - 80s 339ms/step - loss: 0.5148 - accuracy: 0.7507 - val_loss: 0.4097 - val_accuracy: 0.8156\n",
            "Epoch 5/20\n",
            "235/235 [==============================] - 80s 343ms/step - loss: 0.3901 - accuracy: 0.8323 - val_loss: 0.4020 - val_accuracy: 0.8488\n",
            "Epoch 6/20\n",
            "235/235 [==============================] - 81s 343ms/step - loss: 0.3546 - accuracy: 0.8493 - val_loss: 0.3532 - val_accuracy: 0.8501\n",
            "Epoch 7/20\n",
            "235/235 [==============================] - 81s 344ms/step - loss: 0.3337 - accuracy: 0.8589 - val_loss: 0.3442 - val_accuracy: 0.8528\n",
            "Epoch 8/20\n",
            "235/235 [==============================] - 83s 351ms/step - loss: 0.3221 - accuracy: 0.8638 - val_loss: 0.3392 - val_accuracy: 0.8556\n",
            "Epoch 9/20\n",
            "235/235 [==============================] - 78s 331ms/step - loss: 0.3110 - accuracy: 0.8718 - val_loss: 0.3356 - val_accuracy: 0.8551\n",
            "Epoch 10/20\n",
            "235/235 [==============================] - 78s 334ms/step - loss: 0.3071 - accuracy: 0.8742 - val_loss: 0.3346 - val_accuracy: 0.8560\n",
            "Epoch 11/20\n",
            "235/235 [==============================] - 79s 335ms/step - loss: 0.3047 - accuracy: 0.8770 - val_loss: 0.3528 - val_accuracy: 0.8572\n",
            "Epoch 12/20\n",
            "235/235 [==============================] - 78s 333ms/step - loss: 0.2969 - accuracy: 0.8776 - val_loss: 0.3558 - val_accuracy: 0.8560\n",
            "Epoch 13/20\n",
            "235/235 [==============================] - 78s 333ms/step - loss: 0.2882 - accuracy: 0.8813 - val_loss: 0.3498 - val_accuracy: 0.8555\n",
            "Epoch 14/20\n",
            "235/235 [==============================] - 83s 354ms/step - loss: 0.2906 - accuracy: 0.8823 - val_loss: 0.3428 - val_accuracy: 0.8524\n",
            "Epoch 15/20\n",
            "235/235 [==============================] - 87s 371ms/step - loss: 0.2852 - accuracy: 0.8817 - val_loss: 0.3315 - val_accuracy: 0.8568\n",
            "Epoch 16/20\n",
            "235/235 [==============================] - 82s 348ms/step - loss: 0.2816 - accuracy: 0.8838 - val_loss: 0.3278 - val_accuracy: 0.8589\n",
            "Epoch 17/20\n",
            "235/235 [==============================] - 81s 346ms/step - loss: 0.2791 - accuracy: 0.8876 - val_loss: 0.3363 - val_accuracy: 0.8562\n",
            "Epoch 18/20\n",
            "235/235 [==============================] - 80s 342ms/step - loss: 0.2744 - accuracy: 0.8924 - val_loss: 0.3455 - val_accuracy: 0.8519\n",
            "Epoch 19/20\n",
            "235/235 [==============================] - 82s 350ms/step - loss: 0.2734 - accuracy: 0.8896 - val_loss: 0.3434 - val_accuracy: 0.8583\n",
            "Epoch 20/20\n",
            "235/235 [==============================] - 82s 348ms/step - loss: 0.2712 - accuracy: 0.8912 - val_loss: 0.3309 - val_accuracy: 0.8578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0b61f2ae48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGk666NLHnxY"
      },
      "source": [
        "**6. Evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOvPg7vCG8jS",
        "outputId": "359a4f39-8b10-4344-a3bf-c2687a3c933c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = model.evaluate(test_encode, y_test)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 42s 53ms/step - loss: 0.3280 - accuracy: 0.8581\n",
            "[0.328000009059906, 0.8580800294876099]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GirBl7oWP0i8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}